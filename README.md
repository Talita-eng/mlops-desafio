# mlops-desafio


ğŸš€ Desafio de MLOps â€“ Progressivo por NÃ­vel de Dificuldade

Este desafio foi criado para guiar o aprendizado em MLOps, aumentando gradualmente o nÃ­vel de complexidade.
O objetivo Ã© construir uma aplicaÃ§Ã£o de serving de modelos de machine learning utilizando Flask ou FastAPI, com gerenciamento de modelos pelo MLflow.


---

ğŸ”¹ NÃ­vel 1 â€“ AplicaÃ§Ã£o BÃ¡sica de InferÃªncia

ğŸ“Œ Objetivo:

Criar uma aplicaÃ§Ã£o Flask/FastAPI que:

Carregue em memÃ³ria um modelo de classificaÃ§Ã£o treinado com alguma biblioteca do scikit-learn.

Disponibilize um endpoint POST /predict que receba dados e retorne a inferÃªncia.




---

ğŸ”¹ NÃ­vel 2 â€“ Treinamento sob Demanda

ğŸ“Œ Objetivo:

Adicionar um novo endpoint POST /train que permita treinar o mesmo modelo, mas utilizando outro dataset fornecido pelo cliente.

O modelo treinado deve ser atualizado na aplicaÃ§Ã£o e utilizado nas prÃ³ximas inferÃªncias.



---

ğŸ”¹ NÃ­vel 3 â€“ OpÃ§Ãµes de Modelos e Datasets

ğŸ“Œ Objetivo:

Expandir o endpoint /train para permitir que o cliente escolha:

O tipo de modelo de classificaÃ§Ã£o (ex.: RandomForestClassifier, LogisticRegression, SVC, etc.).

O dataset a ser utilizado.


O MLflow deve ser integrado para versionar e registrar os experimentos de treinamento.



---

ğŸ”¹ NÃ­vel 4 â€“ Listagem de Modelos

ğŸ“Œ Objetivo:

Criar um endpoint GET /models que liste todos os modelos jÃ¡ treinados e armazenados no MLflow.

A resposta deve incluir informaÃ§Ãµes como: nome, versÃ£o e data de criaÃ§Ã£o.



---

ğŸ”¹ NÃ­vel 5 â€“ Troca de Modelo em MemÃ³ria

ğŸ“Œ Objetivo:

Criar um endpoint POST /use-model que permita ao usuÃ¡rio carregar em memÃ³ria qualquer modelo listado no MLflow.

ApÃ³s a troca, o endpoint /predict deve utilizar o novo modelo ativo para realizar inferÃªncias.



---

ğŸ”¹ NÃ­vel 6 â€“ ValidaÃ§Ã£o de ParÃ¢metros de InferÃªncia

ğŸ“Œ Objetivo:

No endpoint /predict, validar se o cliente estÃ¡ enviando os parÃ¢metros corretos para o modelo carregado.

Caso os parÃ¢metros estejam incorretos, retornar um erro 400 â€“ Bad Request com um exemplo do formato esperado.



---

ğŸ”¹ NÃ­vel 7 â€“ Deploy e Arquitetura em ProduÃ§Ã£o (AWS)

ğŸ“Œ **Objetivo:**  
Explicar onde e como servir a aplicaÃ§Ã£o em produÃ§Ã£o, apresentando opÃ§Ãµes reais de deploy na AWS e o desenho arquitetural recomendado.

Para servir o modelo em produÃ§Ã£o, a arquitetura ideal depende do volume de requisiÃ§Ãµes, custo esperado e necessidade de escalabilidade.  
Seguindo boas prÃ¡ticas do **AWS Well-Architected Framework**, o fluxo comum inclui um endpoint HTTP (API Gateway) chamando uma camada de computaÃ§Ã£o que executa a inferÃªncia.

ğŸ”¸ **OpÃ§Ã£o 1 â€” AWS Lambda + API Gateway (baixa/mÃ©dia demanda)**  
SoluÃ§Ã£o mais simples e barata, executa sob demanda.

**Quando usar:**  
- payload pequeno  
- inferÃªncia rÃ¡pida (< 10â€“15 s)  
- carga esporÃ¡dica  

**Fluxo:**  
**Cliente â†’ API Gateway â†’ Lambda â†’ Modelo no S3/MLflow**

**Vantagens:**  
- Escalabilidade automÃ¡tica  
- Custo por execuÃ§Ã£o  
- Zero gestÃ£o de servidores  

**LimitaÃ§Ãµes:**  
- MÃ¡ximo **10 GB** em `/tmp`  
- MÃ¡ximo **15 min** de execuÃ§Ã£o  
- Cold start com modelos grandes  

---

ğŸ”¸ **OpÃ§Ã£o 2 â€” ECS Fargate + API Gateway (produÃ§Ã£o contÃ­nua)**  
O modelo roda em um container FastAPI sempre ativo.

**Quando usar:**  
- volume moderado/alto  
- modelo precisa ficar carregado em memÃ³ria  
- latÃªncia baixa Ã© prioridade  

**Fluxo:**  
**Cliente â†’ API Gateway â†’ ECS/Fargate â†’ Container FastAPI â†’ MLflow/S3**

**Vantagens:**  
- Baixa latÃªncia  
- Escala automÃ¡tica  
- Ã“timo para cargas constantes  

---

ğŸ”¸ **OpÃ§Ã£o 3 â€” Amazon SageMaker (MLOps avanÃ§ado)**  
SoluÃ§Ã£o completa para todo o ciclo de Machine Learning.

**Quando usar:**  
- monitoramento e drift detection  
- autoscaling especializado  
- deploy blue/green  
- versionamento robusto  

**Fluxo:**  
**Cliente â†’ API Gateway â†’ SageMaker Endpoint**

**Vantagens:**  
- Autoscaling de ML nativo  
- MÃ©tricas integradas  
- Deploy profissional sem esforÃ§o  

---

ğŸ”¹ NÃ­vel 8 â€“ SeparaÃ§Ã£o de Treinamento e InferÃªncia + AWS Lambda

ğŸ“Œ **Objetivo:**  
Mostrar como separar corretamente **treinamento** e **inferÃªncia**, requisitos mÃ­nimos e como rodar inferÃªncia em Lambda.

---

ğŸ”¸ **SeparaÃ§Ã£o entre Treinamento e InferÃªncia**

ğŸ§  Treinamento (Training Pipeline)  
Exige mais CPU/GPU/memÃ³ria e nÃ£o deve rodar na aplicaÃ§Ã£o de inferÃªncia.

**ServiÃ§os recomendados:**  
- AWS SageMaker Training Jobs  
- AWS Batch  
- EC2 Spot (barato)  
- ECS Fargate (menos comum)

**SaÃ­das do treinamento:**  
- âœ” Modelo final (`.pkl` ou diretÃ³rio MLflow)  
- âœ” Metadados  
- âœ” Registro no MLflow Model Registry  
- âœ” Upload no S3  

---

âš¡ InferÃªncia (Serving)  
Precisa ser rÃ¡pida, estÃ¡vel e de baixo custo.  
Nunca deve treinar nada â€” apenas carregar versÃµes do S3/MLflow.

**ServiÃ§os recomendados:**  
- AWS Lambda  
- ECS Fargate  
- SageMaker Endpoint  

---

 **Requisitos mÃ­nimos para InferÃªncia**

**Lambda**  
- **512 MB â€“ 1024 MB** RAM recomendados  
- modelos menores que **200 MB**  
- inferÃªncia mÃ©dia < **3 s**  

**ECS Fargate**  
- 0.5 vCPU + **1 GB RAM** mÃ­nimo  
- ideal para modelo sempre carregado  

**SageMaker Endpoint**  
- instÃ¢ncia mÃ­nima: **ml.t2.medium**  
- ideal para baixa latÃªncia  

---

ğŸ”¸ **Como usar Lambda para este projeto**

Lambda executa sua FastAPI usando ferramentas como:  
- Mangum  
- AWS Lambda Powertools  
- Zappa  
- Lambyda  

**Fluxo:**  
**Treinamento â†’ S3/MLflow Registry â†’ API Gateway â†’ Lambda â†’ Modelo carregado do S3**

**Passos:**  
1. Empacotar FastAPI + MLflow + dependÃªncias (ZIP ou container).  
2. Lambda baixa o modelo para `/tmp`.  
3. O modelo Ã© carregado na primeira execuÃ§Ã£o (cold start).  
4. API Gateway expÃµe o endpoint para o cliente.  

**Vantagens:**  
- Infra barata  
- Escalabilidade automÃ¡tica  
- Simples de manter  

**Desvantagens:**  
- Cold start  
- Limite de memÃ³ria e tempo  

---

ğŸ”¹ **Resumo dos NÃ­veis 7 e 8**

**NÃ­vel 7 â€“ Deploy**  
- Deploy recomendado: **API Gateway + Lambda/ECS/SageMaker**  
- **Baixa demanda â†’ Lambda**  
- **Demanda contÃ­nua â†’ ECS**  
- **MLOps completo â†’ SageMaker**  

**NÃ­vel 8 â€“ Arquitetura**  
- Separar completamente **treinamento** e **inferÃªncia**  
- InferÃªncia leve: **Lambda (512â€“1024MB)** ou **ECS (1GB RAM)**  
- Lambda usa FastAPI + modelo carregado do S3/MLflow via API Gateway  

---

âš¡ Dicas TÃ©cnicas:

Use MLflow Tracking para registrar e versionar modelos.

Use pydantic (se optar por FastAPI) para validaÃ§Ã£o de payloads.

Estruture a aplicaÃ§Ã£o em camadas para facilitar a evoluÃ§Ã£o entre os nÃ­veis.
